# 2. Basic Scrapy Spiders 

### Source Code 
1. [quotes_spider](../src/quotes_spider)


### Terminal Scrapy Commands 
* `>>> scrapy startproject <project_name>`: start a new project
* file `scrapy.cfg` is the default configuration file for Scrapy project  
* file `settings.py` stores the settings for the project and can be modified
* `>>> scrapy genspider <spider_file_name> <web_URL>`: generate a spider python file using pre-defined template with the specified URL 
* `>>> scrapy list`: list all spiders in this project 
* `>>> scrapy shell`: open an interactive shell terminal for Scrapy (for testing & initial trials)
* `>>> scrapy crawl <spider_name>`: run a Scrapy spider 


### Scrapy Spider File Details 
* var `name`: unique in a project  
* var `allowed_domains`: a list of allowed domains 
* var `start_urls`: a list of URLs to start with 
* function `parse()`: In the parse function, you parse the response (web page) and return either dicts with extracted data, Item objects, Request objects, or an iterable of these objects.


### Scrapy Shell Functions & Interactions 
* `fetch("<web_URL>")`: fetch a response object from URL  
* `response.css()` `response.xpath()`: select data points from the HTML structure  
* `response.xpath().extract()`: list of plain text representations  
* `response.xpath('//*[@id=""]')` `response.xpath('//*[@class=""]')`: id elements and class elements selector 
* 


### Tips and Tricks
* To avoid scraping `robot.text` to further rules, set the var `ROBOTSTXT_OBEY` in `settings.py` to False. 
* If we don't have internet connection or incorrect URL, a `DNSLookupError` will be raised. 
