# 2. Basic Scrapy Spiders 

### Source Code 
1. [quotes.py](../src/quotes_spider/quotes_spider/spiders/quotes.py)
2. [example.py](../src/quotes_spider/quotes_spider/spiders/example.py)


### Terminal Scrapy Commands 
* `>>> scrapy startproject <project_name>`: start a new project
* file `scrapy.cfg` is the default configuration file for Scrapy project  
* file `settings.py` stores the settings for the project and can be modified
* `>>> scrapy genspider <spider_file_name> <web_URL>`: generate a spider python file using pre-defined template with the specified URL 
* `>>> scrapy list`: list all spiders in this project 
* `>>> scrapy shell`: open an interactive shell terminal for Scrapy (for testing & initial trials)
* `>>> scrapy crawl <spider_name>`: run a Scrapy spider 
* `>>> scrapy crawl <spider_name> -o <output.format>`: output yield data to a file; format can be `csv` `json` `xml` 
* `>>> scrapy crawl <spider_name> -s <settings>`: overwrite settings when running a spider 
* `>>> scrapy runspider <spider.py>`: run quick, small spider files

### Scrapy Spider File Details 
* var `name`: unique in a project  
* var `allowed_domains`: a list of allowed domains 
* var `start_urls`: a list of URLs to start with 
* function `parse()`: In the parse function, you parse the response (web page) and return either dicts with extracted data, Item objects, Request objects, or an iterable of these objects.


### Scrapy Shell Functions & Interactions 
* `fetch("<web_URL>")`: fetch a response object from URL  
* `response.css()` `response.xpath()`: select data points from the HTML structure  
* `response.xpath().extract()`: list of plain text representations  
* `response.xpath('//*[@id=""]')` `response.xpath('//*[@class=""]')`: id elements and class elements selector 
* using `yield` statement for output will make it easy for generating formatted output data files (JSON, XML, CSV) 
 


### Tips and Tricks
* To avoid scraping `robot.text` to further rules, set the var `ROBOTSTXT_OBEY` in `settings.py` to False. 
* If we don't have internet connection or incorrect URL, a `DNSLookupError` will be raised. 
